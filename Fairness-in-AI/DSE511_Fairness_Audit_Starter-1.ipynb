{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c5962c1",
   "metadata": {},
   "source": [
    "# DSE511 • Fairness Audit Mini‑Project (Starter Notebook)\n",
    "\n",
    "**Goal:** Train a baseline classifier on the synthetic admissions dataset, compute group fairness metrics, explore interpretability (SHAP), and try one mitigation. Keep code clean and reproducible.\n",
    "\n",
    "**Deliverables:** This filled notebook + a short PDF write‑up (≤ 2 pages) with optional figures and interpretation.\n",
    "\n",
    "**Dataset:** `admissions_synth_v1.csv` (bias‑infused, synthetic). Data dictionary provided separately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d449ce11",
   "metadata": {},
   "source": [
    "## 0) Setup & Reproducibility\n",
    "\n",
    "- Use fixed random seeds.\n",
    "- Avoid relying on global state.\n",
    "- Keep cells idempotent.\n",
    "- Python ≥ 3.9 recommended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600851aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "RANDOM_STATE = 7\n",
    "np.random.seed(RANDOM_STATE)\n",
    "DATA_PATH = Path('admissions_synth_v1.csv')\n",
    "assert DATA_PATH.exists(), 'Place admissions_synth_v1.csv in this directory.'\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202a083",
   "metadata": {},
   "source": [
    "## 1) Load Data & Quick EDA\n",
    "Use **only** matplotlib for plots in this notebook (no seaborn), per style guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d75a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe53a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class balance\n",
    "admit_rate = df['admit'].mean()\n",
    "print('Overall admit rate:', round(admit_rate, 3))\n",
    "\n",
    "# Simple bar chart of admit rate by race\n",
    "by_race = df.groupby('race')['admit'].mean().sort_values()\n",
    "plt.figure()\n",
    "by_race.plot(kind='bar')\n",
    "plt.xlabel('Race')\n",
    "plt.ylabel('Admit rate')\n",
    "plt.title('Admit rate by race')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046c2020",
   "metadata": {},
   "source": [
    "## 2) Train/Test Split & Baseline Model\n",
    "Baseline: Logistic Regression (you may also try RandomForest or GradientBoosting later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa762fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "\n",
    "y = df['admit'].astype(int).values\n",
    "X = df.drop(columns=['admit'])\n",
    "\n",
    "num_cols = ['gpa','sat_total','ap_courses','rec_score','interview_score',\n",
    "            'ecs_hours_per_week','income_quintile','in_state','legacy','first_gen']\n",
    "cat_cols = ['gender','race','intended_major']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)\n",
    "])\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('prep', pre),\n",
    "    ('lr', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "proba = clf.predict_proba(X_test)[:,1]\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "print('ROC-AUC:', round(roc_auc_score(y_test, proba), 3))\n",
    "print('Accuracy:', round(accuracy_score(y_test, pred), 3))\n",
    "print('Confusion matrix @0.5:\\n', confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b0784d",
   "metadata": {},
   "source": [
    "## 3) Group Fairness Metrics\n",
    "Compute **Demographic Parity (selection rate)**, **True Positive Rate (TPR)**, and **False Positive Rate (FPR)** by group, then summarize gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def group_metrics(y_true, y_pred, groups):\n",
    "    out = {}\n",
    "    for g in np.unique(groups):\n",
    "        mask = (groups == g)\n",
    "        yt, yp = y_true[mask], y_pred[mask]\n",
    "        sel = yp.mean()\n",
    "        tp = ((yp==1) & (yt==1)).sum()\n",
    "        fn = ((yp==0) & (yt==1)).sum()\n",
    "        fp = ((yp==1) & (yt==0)).sum()\n",
    "        tn = ((yp==0) & (yt==0)).sum()\n",
    "        tpr = tp/(tp+fn) if (tp+fn)>0 else np.nan\n",
    "        fpr = fp/(fp+tn) if (fp+tn)>0 else np.nan\n",
    "        out[g] = {'selection_rate': sel, 'TPR': tpr, 'FPR': fpr, 'n': mask.sum()}\n",
    "    return pd.DataFrame(out).T\n",
    "\n",
    "# Example: race and first_gen\n",
    "race_test = X_test['race'].values\n",
    "gm_race = group_metrics(y_test, pred, race_test)\n",
    "display(gm_race)\n",
    "\n",
    "fg_test = X_test['first_gen'].values\n",
    "gm_fg = group_metrics(y_test, pred, fg_test)\n",
    "display(gm_fg)\n",
    "\n",
    "# Gap summaries\n",
    "def summarize_gaps(gdf):\n",
    "    return pd.Series({\n",
    "        'DP_diff (max-min selection)': gdf['selection_rate'].max() - gdf['selection_rate'].min(),\n",
    "        'EO_diff (max-min TPR)': gdf['TPR'].max() - gdf['TPR'].min(),\n",
    "        'AvgOdds_diff (avg of FPR/TPR gaps)': 0.5*((gdf['TPR'].max()-gdf['TPR'].min()) + (gdf['FPR'].max()-gdf['FPR'].min()))\n",
    "    })\n",
    "\n",
    "print('Race gaps:')\n",
    "display(summarize_gaps(gm_race))\n",
    "print('First‑gen gaps:')\n",
    "display(summarize_gaps(gm_fg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc480f3a",
   "metadata": {},
   "source": [
    "## 4) Threshold Exploration (Post‑processing idea)\n",
    "Explore group‑specific thresholds to equalize TPR or selection rates, then re‑report accuracy and fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0888bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_by_group(proba, base_thresh, groups, policy='equal_tpr'):\n",
    "    # Simple illustrative approach: search thresholds per group on validation-like split\n",
    "    # Here, we just nudge thresholds by z-score of group mean proba as a toy method.\n",
    "    th = {}\n",
    "    unique = np.unique(groups)\n",
    "    for g in unique:\n",
    "        m = proba[groups==g].mean()\n",
    "        s = proba.mean()\n",
    "        # if group probas are lower than overall, reduce threshold a bit, else increase\n",
    "        th[g] = float(base_thresh - 0.1*(s - m))\n",
    "    return th\n",
    "\n",
    "base = 0.5\n",
    "race_groups = X_test['race'].values\n",
    "tmap = threshold_by_group(proba, base, race_groups)\n",
    "pred_gp = np.array([1 if p>=tmap[g] else 0 for p,g in zip(proba, race_groups)])\n",
    "\n",
    "print('Accuracy (group thresholds):', round(accuracy_score(y_test, pred_gp), 3))\n",
    "gm_race_adj = group_metrics(y_test, pred_gp, race_groups)\n",
    "display(gm_race_adj)\n",
    "display(summarize_gaps(gm_race_adj))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf7fc1d",
   "metadata": {},
   "source": [
    "## 5) Interpretability (SHAP)\n",
    "Use Kernel SHAP over the pipeline prediction function. Keep background small for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d18fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Wrapper to pass raw DataFrame to the fitted pipeline\n",
    "def f_predict(Xraw):\n",
    "    # Ensure DataFrame with same columns/order as training\n",
    "    if not isinstance(Xraw, pd.DataFrame):\n",
    "        Xraw = pd.DataFrame(Xraw, columns=X_test.columns)\n",
    "    return clf.predict_proba(Xraw)[:,1]\n",
    "\n",
    "bg = X_train.sample(200, random_state=RANDOM_STATE)\n",
    "explainer = shap.KernelExplainer(f_predict, bg)\n",
    "X_eval = X_test.sample(200, random_state=RANDOM_STATE)\n",
    "shap_vals = explainer.shap_values(X_eval)\n",
    "\n",
    "shap.summary_plot(shap_vals, X_eval, feature_names=X_eval.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad003ae8",
   "metadata": {},
   "source": [
    "## 6) Mitigation (Choose ONE)\n",
    "\n",
    "Try **one** of the options below and re‑audit fairness & utility:\n",
    "\n",
    "1. **Pre‑processing (reweighing / resampling):** upsample positives for under‑selected groups.\n",
    "2. **In‑processing (approximate):** add group‑aware sample weights to the loss (e.g., weight first‑gen or specific race groups).\n",
    "3. **Post‑processing:** adopt group‑specific thresholds to equalize TPR or selection rate.\n",
    "\n",
    "Document: What changed? What trade‑offs occurred?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee87c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: simple pre‑processing via group‑aware upsampling (toy illustration)\n",
    "from sklearn.utils import resample\n",
    "\n",
    "train = X_train.copy()\n",
    "train['admit'] = y_train\n",
    "minority_mask = (train['first_gen']==1)\n",
    "maj = train[~minority_mask]\n",
    "minr = train[minority_mask]\n",
    "\n",
    "# Upsample first‑gen positives to reduce selection/TPR gaps (toy; tune as needed)\n",
    "minr_pos = minr[minr['admit']==1]\n",
    "if len(minr_pos) > 0:\n",
    "    minr_pos_up = resample(minr_pos, replace=True, n_samples=min(len(maj), len(minr_pos)*2), random_state=RANDOM_STATE)\n",
    "    train_up = pd.concat([maj, minr, minr_pos_up], ignore_index=True)\n",
    "else:\n",
    "    train_up = train.copy()\n",
    "\n",
    "y_train_up = train_up['admit'].astype(int).values\n",
    "X_train_up = train_up.drop(columns=['admit'])\n",
    "\n",
    "clf_up = Pipeline([\n",
    "    ('prep', pre),\n",
    "    ('lr', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "clf_up.fit(X_train_up, y_train_up)\n",
    "proba_up = clf_up.predict_proba(X_test)[:,1]\n",
    "pred_up = (proba_up >= 0.5).astype(int)\n",
    "\n",
    "print('Mitigated ROC-AUC:', round(roc_auc_score(y_test, proba_up), 3))\n",
    "print('Mitigated Accuracy:', round(accuracy_score(y_test, pred_up), 3))\n",
    "gm_fg_up = group_metrics(y_test, pred_up, X_test['first_gen'].values)\n",
    "display(gm_fg_up)\n",
    "display(summarize_gaps(gm_fg_up))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485ee3b",
   "metadata": {},
   "source": [
    "## 7) Reflection (200–300 words)\n",
    "\n",
    "- Which **fairness definition(s)** did you prioritize and why?\n",
    "- Is your model **trustworthy** for this use case? Under which constraints or policies?\n",
    "- What risks or unintended harms remain after mitigation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a66548",
   "metadata": {},
   "source": [
    "---\n",
    "### Appendix: Notes\n",
    "- You may also try `RandomForestClassifier` or `HistGradientBoostingClassifier`.\n",
    "- For calibration by group, consider reliability curves (matplotlib only).\n",
    "- Keep the same **test split** across experiments for comparability if you can.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebeb2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
